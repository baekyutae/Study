
# json api란?


넓게는 JSON 포맷으로 통신하는 모든 API를 뜻하지만, 좁은 의미로는 `jsonapi.org` 에서 정의한 '데이터 교환 표준 스펙

단순히 데이터를 주고받는 것을 넘어, **리소스 간의 관계(Relationships)를 표현**하고, 요청 횟수를 줄이는(Compound Documents) 등 효율적인 통신을 위한 **엄격한 규칙**을 따르는 API

왜 엄격한 규칙?

데이터 전송시 구조가 규칙으로 정해져 있음

비교

일반 API (구조가 달라질 수 있음)

``` json
// 유저 응답 
{
  "user": { "name": "철수", "age": 25 },
  "status": "OK"
}

// 상품 응답 
{
  "items": [
    { "productName": "노트북", "price": 1000000 }
  ],
  "message": "성공"
}
```


json api

데이터는 무조건 data안에 속성은 attributes 안에 넣음

```json
// 유저 응답
{
  "data": {
    "type": "users",
    "id": "1",
    "attributes": {
      "name": "철수", // <-- 이름이 여기 있음
      "age": 25
    }
  }
}

// 상품 응답
{
  "data": {
    "type": "products",
    "id": "100",
    "attributes": {
      "name": "노트북", // <-- 이름이 똑같은 위치에 있음
      "price": 1000000
    }
  }
}
```

# 장점 

1. 코드 재사용성이 높다.
	데이터 종류마다 파싱 코드를 짤필요 없이 하나의 함수로 파싱하는 데이터 대상만 바꾸면 됨

2. 'N+1 문제' 해결

	**n+1 문제란?**
	목록(1번)을 조회한 후, 각 데이터에 연관된 정보(프로필, 댓글 등)를 얻기 위해 데이터 개수만큼(N번) 추가적인 DB 쿼리를 반복해서 날리는 비효율적인 상황<br>
	1은 목록을 가져오는 최초의 쿼리, n은 목록의 데이터 갯수만큼 날린 쿼리문<br>
	json api는 이걸 include 파라미터로 해결한다
	\+ Compound Documents(복합문서)라 부름<br>
	- **기존 방식 (N+1 발생):** `GET /articles` (게시글만 줘) ➔ 나중에 작성자 정보 없어서 또 요청해야 함
    
	- **JSON API 방식 (해결!):** **`GET /articles?include=author`** (게시글 주면서 작성자도 같이 담음) ➔ **단 1번의 요청**으로 끝난다<br>
	예시
	- **`data`**: 요청한 **메인 데이터** (게시글)
    
	- **`included`**: 같이 달라고 한 **관련 데이터** (작성자)<br>
	```json
	{
  // 1. 메인 데이터 (게시글 목록)
  "data": [
    {
      "type": "articles",
      "id": "1",
      "attributes": { "title": "JSON API 학습법" },
      "relationships": {
        "author": {
          "data": { "type": "people", "id": "9" } // <-- "작성자는 9번이야"라고 가리킴
        }
      }
    },
    {
      "type": "articles",
      "id": "2",
      "attributes": { "title": "N+1 문제 해결" },
      "relationships": {
        "author": {
          "data": { "type": "people", "id": "9" } // <-- "이것도 9번이 썼음"
        }
      }
    }
  ],

  // 2. 포함된 데이터 (여기에 작성자 정보가 딱! 들어옴)
  "included": [
    {
      "type": "people",
      "id": "9", // <-- 위에서 가리킨 9번의 실체
      "attributes": {
        "name": "주니어개발자",
        "twitter": "@junior_dev"
      }
    }
  ]
}
	```
	
	
	이걸 활용해서 json api는 꼬리에 꼬리를 무는 데이터도 한번의 요청으로 가져올 수 있음
	
	- **요청:** `GET /articles?include=author,comments.author`
    
	- **의미:**
    
    1. 게시글 줘.
        
    2. 그 글 쓴 작성자(`author`)도 줘.
        
    3. 그 글의 댓글(`comments`)도 줘.
        
    4. 심지어 그 댓글을 쓴 사람(`comments.author`)까지 줘

# 단점

JSON API 스펙은 `data > attributes` 처럼 **중첩된 구조(Depth)가 강제**되기 때문에,

1. 일반적인 API보다 **데이터를 꺼내는 코드가 길어지고,
    
2. 중간에 데이터가 비어있을 경우 **참조 에러(Null Pointer Exception)로 인해 앱이 비정상 종료될 위험**이 높아진다.<br>
최근에는 json api의 단점을 보완하고자 GraphQL, Pragmatic REST 을 사용하기도 함



# 응답/요청 구조 설계 패턴<br>

## 1. Envelope 패턴<br>
데이터만 보내는게 아니라 항상 똑같은 구조에 담아서 보내는 방식

패턴 적용하기 전
```JSON
// 성공했을 때 (배열 반환)
[ { "id": 1, "name": "철수" } ]

// 실패했을 때 (객체 반환)
{ "message": "에러 발생!" }
```

패턴 적용후
```JSON
// 성공했을 때
{
  "status": "SUCCESS",
  "data": [ { "id": 1, "name": "철수" } ], // 데이터는 여기!
  "error": null
}

// 실패했을 때
{
  "status": "ERROR",
  "data": null,
  "error": { "code": "E400", "message": "잘못된 요청" } // 에러는 여기!
}
```


공통 응답 필드 (Response Wrapper)

| **필드명**         | **타입**         | **설명**                                |
| --------------- | -------------- | ------------------------------------- |
| **`result`**    | String / Enum  | 요청 결과 상태 (`SUCCESS`, `FAIL`, `ERROR`) |
| **`data`**      | Object / Array | 성공 시 실제 데이터 (실패 시 `null`)             |
| **`message`**   | String         | 사용자에게 보여줄 간단한 메시지 (선택 사항)             |
| **`errorCode`** | String         | 실패 시 에러 코드 (성공 시 `null`)              |

성공 응답시
data에 내용이 들어가고 에러 관련 필드는 비워둔다.

```json
{
  "result": "SUCCESS",
  "data": {
    "userId": 101,
    "username": "junior_dev",
    "email": "dev@example.com"
  },
  "message": null,     // 성공했으니 메시지 불필요 (또는 "조회 성공")
  "errorCode": null    // 에러 없으니까 null
}
```

에러 응답시
HTTP 상태 코드(400, 500)와 별개로 **구체적인 에러 사유**를 알려줘야 한다.

```json
{
  "result": "FAIL",
  "data": null,        // 실패했으니 데이터 없음
  "message": "이메일 형식이 올바르지 않습니다.", // 사용자에게 보여줄 메시지
  "errorCode": "USER_001_INVALID_EMAIL"     // 프론트엔드가 처리할 고유 코드
}
```


### 장점<br>
1) 응답 본문의 고정된 영역에 전체 페이지 수나 서버 시간 같은 부가 정보를 담을 수 있어, 데이터의 확장성이 뛰어나다.
   
2) 에러 처리를 획일화 할수 있다.
	http 상태 코드에만 의존하지 않고 비즈니스 로직 결과를 명확히 전달 가능, response.result 값을 확인해 `SUCCESS`면 -> 화면 갱신 `FAIL`이면 -> 에러 메시지 팝업

### 단점

1) 중간서버 캐싱 오류: 브라우저나 CDN이 에러 메시지를 포함한 `200 OK` 응답을 정상 데이터로 오인해 저장해버리면, 이후 사용자에게 계속해서 에러 화면이 노출될 수 있다.
   
2) 불필요한 데이터 낭비: 아주 작은 데이터를 보낼 때도 고정된 Envelope 규격(status, message 등)을 모두 포함해야 하므로 네트워크 트래픽 효율이 떨어질 수 있다.

## 2. Bare 패턴<br>
Envelope(봉투)이라는 개념 자체를 쓰지 않고 데이터만 바로 반환하는 가장 표준적인 REST 방식

응답의 최상위에 실제 데이터(객체나 배열)가 바로 나온다.


## 3. HATEOAS(헤이티오스) 패턴<br>
데이터와 함께 "이 데이터로 다음에 할 수 있는 행동(Link)"을 같이 알려주는 방식


## 4. BFF(Backend For Frontend) 패턴<br>
클라이언트 종류(웹, iOS, Android)에 따라 **서버가 응답 구조를 각각 다르게 다듬어서** 내려주는 패턴



**위 패턴들은 특정 포맷(JSON)에 종속된 것이 아니라 REST API 아키텍처 전반에 적용 가능한 설계이다.**


# Pagination 전략 비교 (Cursor vs Offset)


## pagination이란<br>
대량의 데이터를 한 번에 모두 조회하지 않고, 서버 성능과 사용자 UI/UX를 위해 **데이터를 일정한 크기(Chunk)로 나누어(분할하여) 제공하는 데이터 처리 기법**.

**도입 목적**:

- **서버 및 DB 부하 방지**: 수백만 건의 데이터를 한 번에 메모리에 올리거나 연산하는 것을 막음.
    
- **네트워크 대역폭 절감**: 딱 클라이언트 화면에 그릴 만큼의 필요한 데이터만 전송하여 로딩 속도를 개선함.

### 1. 오프셋 기반 (Offset-based Pagination)

- **동작 원리**: 전체 데이터 중에서 **'앞에서부터 몇 개를 버릴지(OFFSET)'와 '거기서부터 몇 개를 챙길지(LIMIT)'를 데이터베이스에 직접 명령하는 방식임.

	- **LIMIT (가져올 개수)**: 한 화면(페이지)에 몇 개의 게시글을 보여줄 것인지 정함. (예: 10개씩 보기 = `LIMIT 10`)
    
	- **OFFSET (건너뛸 개수)**: 목표 페이지에 도달하기 위해, 앞에서부터 무시하고 지나칠 데이터의 총개수를 의미함.
    
	- 프론트엔드 UI와 백엔드 SQL의 연결 **:<br>
	    - 사용자가 "3페이지 보여줘!"라고 클릭했을 때의 상황을 가정해 봄.
        
	    - 3페이지를 보려면 앞의 1페이지(10개)와 2페이지(10개), 즉 총 20개의 데이터를 건너뛰어야 함.
        
	    - **offset 계산**: `OFFSET = (원하는 페이지 번호 - 1) * LIMIT`
        
	    - **SQL 변환**: `SELECT * FROM posts LIMIT 10 OFFSET 20;`
        
	    - **최종 해석**: "데이터베이스야, 맨 앞부터 20개는 그냥 버리고, 21번째 데이터부터 딱 10개만 챙겨서 나한테 줘!"
    
- **장점**:
    
    - 구현이 직관적이고 매우 단순함.<br>
    - 사용자가 원하는 특정 페이지(예: 7페이지)로 즉시 점프할 수 있음.<br>
    - 전체 페이지 수(`COUNT`) 파악이 용이하여 페이지 네비게이션 UI 구현에 적합함.
        
- **단점**:
    
    - **성능 저하**: 뒤쪽 페이지로 갈수록 건너뛸 데이터까지 DB가 전부 읽고 버려야 하므로 응답 속도가 급격히 느려짐.<br>
    - **데이터 불일치 (Data Drift)**: 조회 도중 새로운 데이터가 추가되거나 삭제되면, 다음 페이지에서 데이터가 중복 노출되거나 누락되는 현상이 발생함.
        
- **적합한 유스케이스**:
    
    - 하단에 페이지 번호 `[1] [2] [3]`이 필요한 전통적인 웹 게시판.
        
    - 데이터 총량이 적거나 잦은 변경이 없는 내부 관리자(Admin) 페이지.
        

### 2. 커서 기반 (Cursor-based Pagination)

- **동작 원리: 전체 데이터를 처음부터 세지 않고, 클라이언트(프론트엔드)가 꽂아둔 **'책갈피(Cursor)' 위치를 DB 인덱스로 즉시 찾아가서 그 뒷부분만 가져오는 방식**임.

	- **Cursor**: 클라이언트가 마지막으로 화면에서 본 데이터의 고유한 기준점. (주로 마지막 게시글의 고유 `id`나 `생성 시간`을 사용함)
    
	- 프론트엔드 UI와 백엔드 SQL의 연결 공식 (무한 스크롤 예시):
    
	    1. **첫 번째 요청**: 사용자가 처음 게시판에 들어오면 커서 없이 "최신 글 10개 줘!"라고 요청함.
        
	    2. **프론트엔드의 기억**: 서버가 준 10개의 글을 화면에 그리고 보니, 맨 마지막(맨 아래) 글의 ID가 '55번'임. 프론트엔드는 이 55번을 다음 책갈피(`cursor=55`)로 기억해 둠.
        
	    3. **두 번째 요청**: 사용자가 스크롤을 맨 아래로 내리면, 프론트엔드는 서버에 "나 방금 55번 글까지 봤어. **55번보다 더 예전에 쓰인 글(작은 숫자)부터** 10개 더 줘!"라고 요청함.
        
	    4. **SQL 변환**: `SELECT * FROM posts WHERE id < 55 ORDER BY id DESC LIMIT 10;`
        
	    5. **최종 해석**: "DB야, 1번 세는게 아닌 55번 위치로 바로 점프 그리고 55번보다 작은 숫자들 중에서 최신순(`DESC`)으로 10개 출력<br>
	- limit + 1<br>
		커서 방식은 전체 글 개수(`COUNT`)를 세지 않기 때문에, 프론트엔드에게 "뒤에 다음 페이지가 또 있는지(`hasNext`)"를 알려주기가 까다로움.<br>
		그래서 백엔드는 프론트엔드가 10개를 달라고 해도 DB에서 11개(`LIMIT 11`)를 꺼내봄.만약 11개가 무사히 조회되면? ➡️ "뒤에 11번째 글이 있으니까 다음 스크롤을 내리면 보여줄 게 또 있네! (`hasNext = true`)"<br>
		11개를 꺼내려 했는데 10개(혹은 그 이하)만 나왔다면? ➡️ **"아, 방금 가져온 게 진짜 마지막 데이터구나! (`hasNext = false`)"**<br>
		최종적으로 프론트엔드에게 넘겨줄 때는 11번째 데이터는 꼬리를 자르고, 약속한 10개만 포장해서 전달함.<br>
- **장점**:
    
    - **압도적인 성능**: DB 인덱스를 타고 해당 위치로 바로 이동하므로 데이터가 수십만 건이라도 항상 일정한 조회 속도를 보장함.
        
    - **데이터 일관성 보장**: 조회 중 데이터가 추가/삭제되어도 명확한 기준점(책갈피)이 있어 중복이나 누락이 발생하지 않음.
        
- **단점**:
    
    - **구현 복잡도 증가**: 정렬 기준이 다양해질수록(예: 좋아요 순 + 최신 순) 고유성을 보장하는 복합 커서를 설계해야 하므로 쿼리와 로직이 까다로워짐.
        
    - **페이지 점프 불가**: 이전/다음 순차적 탐색만 가능하며, 특정 페이지 번호로 단번에 이동할 수 없음.
        
- **적합한 유스케이스**:
    
    - 모바일 앱 등에서 흔히 쓰이는 무한 스크롤(Infinite Scroll) UI.
        
    - 실시간으로 데이터가 빠르게 추가/삭제되는 SNS 피드나 채팅방.
        
    - 대규모 트래픽과 방대한 데이터를 다루는 백엔드 시스템 API.